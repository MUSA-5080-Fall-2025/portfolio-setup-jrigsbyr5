---
title: "Week 10 Notes"
date: "2025-11-10"
---

## Key Concepts Learned

- Logistics Regression

## Coding Techniques

Let‚Äôs build a simple spam detector to understand the mechanics.
Goal: Predict whether email is spam (1) or legitimate (0)
Predictors: - Number of exclamation marks - Contains word ‚Äúfree‚Äù - Email length

## Create example spam detection data
set.seed(123)
n_emails <- 1000

spam_data <- data.frame(
  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !
  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions "free"
  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter
  is_spam = c(rep(1, 100), rep(0, 900))
)

## Look at the data
head(spam_data)

## Fitting the Logistic Model
In R, we use glm() with family = "binomial"

## Fit logistic regression
spam_model <- glm(
  is_spam ~ exclamation_marks + contains_free + length,
  data = spam_data,
  family = "binomial"  # This specifies logistic regression
)

## View results
summary(spam_model)

## Interpreting Coefficients

## Extract coefficients
coefs <- coef(spam_model)
print(coefs)
      (Intercept) exclamation_marks     contains_free            length 
       233.048051         55.944824         46.055006         -1.272668 
## Convert to odds ratios
odds_ratios <- exp(coefs)
print(odds_ratios)
      (Intercept) exclamation_marks     contains_free            length 
    1.627356e+101      1.979376e+24      1.003310e+20      2.800833e-01 


## Making Predictions
The model outputs probabilities:
## Predict probability for a new email
new_email <- data.frame(
  exclamation_marks = 3,
  contains_free = 1,
  length = 150
)

predicted_prob <- predict(spam_model, newdata = new_email, type = "response")
cat("Predicted probability of spam:", round(predicted_prob, 3))
Predicted probability of spam: 1

Calculate metrics at different thresholds
thresholds <- seq(0.1, 0.9, by = 0.1)

metrics_by_threshold <- map_df(thresholds, function(thresh) {
  preds <- ifelse(spam_data$predicted_prob > thresh, 1, 0)
  cm <- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), 
                        positive = "1")
  
  data.frame(
    threshold = thresh,
    sensitivity = cm$byClass["Sensitivity"],
    specificity = cm$byClass["Specificity"],
    precision = cm$byClass["Precision"]
  )
})

## Visualize the trade-off
ggplot(metrics_by_threshold, aes(x = threshold)) +
  geom_line(aes(y = sensitivity, color = "Sensitivity"), size = 1.2) +
  geom_line(aes(y = specificity, color = "Specificity"), size = 1.2) +
  geom_line(aes(y = precision, color = "Precision"), size = 1.2) +
  labs(title = "The Threshold Trade-off",
       subtitle = "As threshold increases, we become more selective",
       x = "Probability Threshold", y = "Metric Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

## Questions & Challenges

How should predictors with different scales be interpreted together?
The model combines count variables (exclamation marks), binary indicators (contains_free), and continuous measures (email length).

How do we assess model performance beyond accuracy?
With imbalanced cases (only 10% spam), accuracy would be misleading, as predicting ‚Äúnot spam‚Äù for all emails would still appear highly accurate.

## Connections to Policy

## Logistic Regression
The solution: Transform the problem
Instead of predicting Y directly, predict the probability that Y = 1
The logistic function constrains predictions between 0 and 1:

Behind the scenes: We work with log-odds, not probabilities directly
Odds: Odds=ùëù1‚àíùëù
Log-Odds (Logit): logit(ùëù)=ln(ùëù1‚àíùëù)
This creates a linear relationship:
ln(ùëù1‚àíùëù)=ùõΩ0+ùõΩ1ùëã1+ùõΩ2ùëã2+...

## Why this matters:
Coefficients are log-odds (like linear regression)
But we interpret as odds ratios when exponentiated: ùëíùõΩ
OR > 1: predictor increases odds of outcome
OR < 1: predictor decreases odds of outcome


## Interpretation:
‚Ä¢	exclamation_marks: Each additional ! multiplies odds of spam by 1.9793761^{24}
‚Ä¢	contains_free: Having ‚Äúfree‚Äù multiplies odds by 1.0033099^{20}
‚Ä¢	length: Each additional character multiplies odds by 0.2801 (shorter = more likely spam)

## Next?
‚Ä¢	If probability = 0.723, is this spam or not?
‚Ä¢	We need to choose a threshold (cutoff)
‚Ä¢	Threshold = 0.5 is common default, but is it the right choice?

## Question: What probability threshold should we use to classify?
‚Ä¢	Threshold = 0.5? (common default)
‚Ä¢	Threshold = 0.3? (more aggressive - flag more as spam)
‚Ä¢	Threshold = 0.7? (more conservative - only flag obvious spam)
The answer depends on:
‚Ä¢	Cost of false positives (marking legitimate email as spam)
‚Ä¢	Cost of false negatives (missing actual spam)
‚Ä¢	These costs are rarely equal

## Calculating Performance Metrics
From the confusion matrix, we derive metrics that emphasize different trade-offs:
Sensitivity (Recall, True Positive Rate):
Sensitivity=ùëáùëÉùëáùëÉ+ùêπùëÅ
‚ÄúOf all actual positives, how many did we catch?‚Äù
Specificity (True Negative Rate):
Specificity=ùëáùëÅùëáùëÅ+ùêπùëÉ
‚ÄúOf all actual negatives, how many did we correctly identify?‚Äù
Precision (Positive Predictive Value):
Precision=ùëáùëÉùëáùëÉ+ùêπùëÉ
‚ÄúOf all our positive predictions, how many were correct?‚Äù

Calculate metrics at different thresholds
thresholds <- seq(0.1, 0.9, by = 0.1)

Selecting the threshold has a huge impact for policy adaptation. The threshold will determine what events are possibly missed and what events are possibly captured.


## Reflection

This lecture clarified that logistic regression is not primarily a classification tool, but a probability-estimation framework. The key insight is that the model‚Äôs core output is uncertainty, not a yes-or-no decision. Classification only occurs after a threshold is chosen, and that choice reflects values, costs, and priorities rather than purely statistical optimization. The threshold analysis emphasized the policy relevance of statistical modeling. Small changes in threshold dramatically shift sensitivity, specificity, and precision, altering who is affected by errors. In practice, this means that model deployment decisions have judgments about risk, fairness, and trade-offs. Logistic regression does not remove subjectivity from decision-making; instead, it makes those choices measurable.


