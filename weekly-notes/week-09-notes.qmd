---
title: "Week 9 Notes"
date: "2025-11-03"
---

## Key Concepts Learned

-   New Rgression Models (i.e negative binomial)
-   New Regression Modeling tools (i.e fishnet/kde)

## Coding Techniques

Local Moranâ€™s I in R: Step by Step
library(spdep)

# Step 1: Create spatial object
fishnet_sp <- as_Spatial(fishnet)

# Step 2: Define neighbors (Queen contiguity)
neighbors <- poly2nb(fishnet_sp, queen = TRUE)

# Step 3: Create spatial weights (row-standardized)
weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)

# Step 4: Calculate Local Moran's I
local_moran <- localmoran(
  fishnet$abandoned_cars,  # Variable of interest
  weights,                  # Spatial weights
  zero.policy = TRUE       # Handle cells with no neighbors
)

# Step 5: Extract components
fishnet$local_I <- local_moran[, "Ii"]      # Local I statistic
fishnet$p_value <- local_moran[, "Pr(z != E(Ii))"]  # P-value
fishnet$z_score <- local_moran[, "Z.Ii"]    # Z-score



# Identifying High-High Clusters (Hotspots)
# Standardize the variable for quadrant classification
fishnet$standardized_value <- scale(fishnet$abandoned_cars)

# Calculate spatial lag (weighted mean of neighbors)
fishnet$spatial_lag <- lag.listw(weights, fishnet$abandoned_cars)
fishnet$standardized_lag <- scale(fishnet$spatial_lag)

# Identify High-High clusters
fishnet$hotspot <- 0  # Default: not a hotspot

# Criteria: 
 1. Value above mean (standardized > 0)
 2. Neighbors above mean (spatial lag > 0)
 3. Statistically significant (p < 0.05)

fishnet$hotspot[
  fishnet$standardized_value > 0 & 
  fishnet$standardized_lag > 0 & 
  fishnet$p_value < 0.05
] <- 1

# Count hotspots
sum(fishnet$hotspot)

# Distribution of Crime Counts
# Typical pattern for crime data
ggplot(fishnet, aes(x = countBurglaries)) +
  geom_histogram(binwidth = 1, fill = "#440154FF", color = "white") +
  labs(
    title = "Distribution of Burglary Counts",
    subtitle = "Most cells have 0-2 burglaries, few have many",
    x = "Burglaries per Cell",
    y = "Number of Cells"
  ) +
  theme_minimal()

# Interpreting Poisson Coefficients
On log scale:
ğ›½1 = change in log(expected count) per unit increase in ğ‘‹1
On count scale (exponentiate):
exp(ğ›½1) = multiplicative effect on expected count
Examples:
ğ›½	exp(ğ›½)	Interpretation
0.14	1.15	15% increase per unit of X
-0.22	0.80	20% decrease per unit of X
0.00	1.00	No effect
0.69	2.00	Doubling per unit of X

# Poisson Regression in R
# Fit Poisson model
model_poisson <- glm(
  countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,
  data = fishnet,
  family = poisson(link = "log")
)

# View results
summary(model_poisson)

# Exponentiate coefficients for interpretation
exp(coef(model_poisson))

# Example output:
#                        exp(coef)
# (Intercept)            0.234
# Abandoned_Cars         1.151
# Abandoned_Cars.nn      0.998
# abandoned.isSig.dist   0.999

# Interpretation:
# - Each additional abandoned car â†’ 15.1% increase in expected burglaries
# - Each meter from nearest abandoned car â†’ 0.2% decrease in expected burglaries

â€¢  If â‰ˆ 1: Poisson is fine
â€¢  If > 1: Overdispersion (common!)
â€¢  If > 2-3: Serious overdispersion â†’ Use Negative Binomial





# Checking Overdispersion in R
# Fit Poisson model
model_pois <- glm(
  countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,
  data = fishnet,
  family = poisson
)

# Calculate dispersion parameter
dispersion <- sum(residuals(model_pois, type = "pearson")^2) / 
               model_pois$df.residual

cat("Dispersion parameter:", round(dispersion, 3), "\n")

# Rule of thumb:
< 1.5: Poisson OK
1.5 - 3: Mild overdispersion, NegBin recommended
> 3: Serious overdispersion, NegBin essential

# Example output:
Dispersion parameter: 4.523
Warning: Serious overdispersion detected


Negative Binomial Regression
Relaxes the variance = mean assumption
Adds dispersion parameter (ğ›¼):
Var(ğ‘Œğ‘–)=ğœ‡ğ‘–+ğ›¼ğœ‡2ğ‘–

Negative Binomial in R
library(MASS)

# Fit Negative Binomial model
model_nb <- glm.nb(
  countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,
  data = fishnet
)

# View results
summary(model_nb)

# Compare to Poisson
AIC(model_pois)  # e.g., 8234.5
AIC(model_nb)    # e.g., 6721.3

# Lower AIC = better fit
 If NegBin AIC much lower â†’ use NegBin

# Extract dispersion parameter (theta)
model_nb$theta  # e.g., 2.47

Interpretation: Significant overdispersion confirmed


# Deviance residuals
plot(model_nb, which = 1)  # Residuals vs. Fitted

# Identify outliers
outliers <- which(abs(residuals(model_nb, type = "deviance")) > 3)

# Influence
influence <- cooks.distance(model_nb)
influential <- which(influence > 4/length(influence))

Creating a Fishnet Grid
library(sf)

# Step 1: Define cell size (in map units - meters for our projection)
cell_size <- 500  # 500m x 500m cells

# Step 2: Create grid over study area
fishnet <- st_make_grid(
  chicago_boundary,
  cellsize = cell_size,
  square = TRUE,
  what = "polygons"
) %>%
  st_sf() %>%
  mutate(uniqueID = row_number())

# Step 3: Clip to study area (remove cells outside boundary)
fishnet <- fishnet[chicago_boundary, ]

# Check results
nrow(fishnet)  # Number of cells
st_area(fishnet[1, ])  # Area of one cell (should be 250,000 mÂ²)


Leave-One-Group-Out Cross-Validation (LOGO-CV)
Principle: Hold out entire spatial groups, not individual cells
Process:
Divide study area into groups (e.g., police districts)
Hold out all cells in District 1
Train model on Districts 2-N
Predict for District 1
Repeat for each district


# LOGO-CV Implementation
# Get unique districts
districts <- unique(fishnet$District)

# Initialize results
cv_results <- list()

# Loop through districts
for (dist in districts) {
  # Split data
  train_data <- fishnet %>% filter(District != dist)
  test_data <- fishnet %>% filter(District == dist)
  
  # Fit model on training data
  model_cv <- glm.nb(
    countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,
    data = train_data
  )
  
  # Predict on test data
  test_data$prediction <- predict(model_cv, test_data, type = "response")
  
  # Store results
  cv_results[[dist]] <- test_data
}

# Combine all predictions
all_predictions <- bind_rows(cv_results


Calculating KDE in R
library(spatstat)

# Step 1: Convert to point pattern (ppp) object
burglary_ppp <- as.ppp(
  X = st_coordinates(burglaries),
  W = as.owin(st_bbox(chicago_boundary))
)

# Step 2: Calculate KDE
kde_surface <- density.ppp(
  burglary_ppp,
  sigma = 1000,  # Bandwidth in meters
  edge = TRUE    # Edge correction
)

# Step 3: Extract values to fishnet cells
fishnet$kde_risk <- raster::extract(
  raster(kde_surface),
  st_centroid(fishnet)
)

# Standardize to 0-1 scale for comparison
fishnet$kde_risk <- (fishnet$kde_risk - min(fishnet$kde_risk, na.rm=T)) / 
                     (max(fishnet$kde_risk, na.rm=T) - min(fishnet$kde_risk, na.rm=T))



## Questions & Challenges

It is difficult to get a significantly accurate prediction when working with crime data, or at least accurate enough to seem appropriate for the high stakes of the result.

## Connections to Policy

The implications of analyzing crime data are far reaching, critical though and an awareness of potential bias must be understood and considered when working with this type of data as the effects are severe.

Comparing Poisson vs. Negative Binomial

Aspect	              Poisson           	                Negative Binomial
Variance assumption	  Var = Mean	                        Var = Î¼ + Î±Î¼Â²
Overdispersion	      Cannot handle	                      Accommodates
Standard errors	      Underestimated if overdispersed	    Correctly estimated
Crime data	          Rarely appropriate	                Usually better


Model Diagnostics for Count Models
Unlike OLS, we donâ€™t use residual plots the same way
Key diagnostics:
1.	Dispersion test (already covered)
2.	Deviance residuals: Should be roughly normal
3.	Pearson residuals: Check for outliers
4.	Cookâ€™s distance: Influential observations
5.	Predicted vs. observed: Visual check

Modeling Workflow
1 Setup & Data Preparation
Load burglaries (point data)
Load abandoned cars (311 calls)
Create fishnet (500m Ã— 500m grid)
Aggregate burglaries to cells

2 Baseline Comparison
Kernel Density Estimation (KDE)
Simple spatial smoothing
What we need to beat!

3 Feature Engineering
Using Abandoned Cars as â€œDisorder Indicatorâ€:
Count in each cell
k-Nearest Neighbors (mean distance to 3 nearest)
LISA (Local Moranâ€™s I - identify hot spots)
Distance to hot spots (significant clusters)

4 Count Regression Models
Fit Poisson regression
Test for overdispersion
Fit Negative Binomial (if needed)
Interpret coefficients


5 Spatial Cross-Validation
Leave-One-Group-Out (LOGO)
Train on n-1 districts
Test on held-out district
Calculate MAE/RMSE

6 Model Comparison
Compare to KDE baseline
Map predictions vs. actual
Analyze errors spatially


## Reflection

While its certainly helpful, I am not confident enough in these analyses, because it just appears that too much is left to question even when we find significant association between predictors.

