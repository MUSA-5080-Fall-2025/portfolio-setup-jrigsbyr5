{
  "hash": "c4c6c684692854aaed7dd7151e4fbbe1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space Time\"\nauthor: \"Josh\"\nformat: html\neditor:\n  markdown: \n    wrap: 72\n---\n\n\n\n# Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nPhiladelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**.\n\nImagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have: - 200 stations across Philadelphia - Limited trucks and staff for moving bikes - 2-3 hours before morning rush hour demand peaks - **The question:** Which stations will run out of bikes by 8:30 AM?\n\nThis lab will build predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.\n\n## Objectives\n\n1.  **Understand panel data structure** for space-time analysis\n2.  **Create temporal lag variables** to capture demand persistence\n3.  **Build multiple predictive models** with increasing complexity\n4.  **Validate models temporally** (train on past, test on future)\n5.  **Analyze prediction errors** in both space and time\n6.  **Engineer new features** based on error patterns\n7.  **Critically evaluate** when prediction errors matter most\n\n------------------------------------------------------------------------\n\n# Part 1\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(CENSUS_API_KEY = \"cb0a6c25010d92d0c374f6a32aa1ea3959229e34\")\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n# Data Import & Preparation\n\n## Load Indego Trip Data (Q2 2025)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q2 2025 data\nindego <- read_csv(\"indego-trips-2025-q2.csv\")\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 365,760\nColumns: 15\n$ trip_id             <dbl> 1164246461, 1164246634, 1164246553, 1164246521, 11…\n$ duration            <dbl> 11, 31, 9, 3, 11, 7, 13, 12, 3, 58, 21, 5, 15, 10,…\n$ start_time          <chr> \"4/1/2025 0:04\", \"4/1/2025 0:04\", \"4/1/2025 0:17\",…\n$ end_time            <chr> \"4/1/2025 0:15\", \"4/1/2025 0:35\", \"4/1/2025 0:26\",…\n$ start_station       <dbl> 3022, 3040, 3396, 3054, 3280, 3301, 3158, 3374, 33…\n$ start_lat           <dbl> 39.95472, 39.96289, 39.92327, 39.96250, 39.93968, …\n$ start_lon           <dbl> -75.18323, -75.16606, -75.18210, -75.17420, -75.21…\n$ end_station         <dbl> 3064, 3100, 3349, 3235, 3349, 3051, 3028, 3154, 33…\n$ end_lat             <dbl> 39.93840, 39.92777, 39.93651, 39.96000, 39.93651, …\n$ end_lon             <dbl> -75.17327, -75.15103, -75.18621, -75.16510, -75.18…\n$ bike_id             <chr> \"31751\", \"14481\", \"02724\", \"24841\", \"25744\", \"3123…\n$ plan_duration       <dbl> 30, 30, 30, 365, 30, 365, 1, 30, 30, 30, 30, 30, 1…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"One Way\", \"One W…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego30\", \"Indego365\", \"…\n$ bike_type           <chr> \"electric\", \"standard\", \"standard\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q2 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q2 2025: 365760 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1743465840 to 1751327820 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 273 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    341060      24700 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     15712     189135     132693          1          4      28215 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  234502   131258 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n2 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n3 2025-04-01 00:17:00 2025-04-01 00:00:00    13 Tue       0       0\n4 2025-04-01 00:20:00 2025-04-01 00:00:00    13 Tue       0       0\n5 2025-04-01 00:25:00 2025-04-01 00:00:00    13 Tue       0       0\n6 2025-04-01 00:40:00 2025-04-01 00:00:00    13 Tue       0       0\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q2 2025\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Question:** What patterns do you see? How does ridership change over time?\n\nThe Patterns Radically shift frequently between two extremes.\n\nIt seems the demand changes by time of day or portion of the week or month. There seems to be a low point mid way through each month.\n\nLet's investigate a comparison between memorial day the 26th of May 2025 to all other Mondays in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Memorial Day date for Q2 2025\nholiday_date <- \"2025-05-26\"\n\n# Trips on Memorial Day\nholiday_trips <- daily_trips %>%\n  filter(date == holiday_date)\n\n# Typical Monday behavior\ntypical_monday <- indego %>%\n  filter(dotw == \"Mon\", date != holiday_date) %>%\n  group_by(date) %>%\n  summarize(trips = n()) %>%\n  summarize(avg_monday_trips = mean(trips))\n\n# Comparison\nprint(holiday_trips)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  date       trips\n  <date>     <int>\n1 2025-05-26  4195\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(typical_monday)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  avg_monday_trips\n             <dbl>\n1            3916.\n```\n\n\n:::\n:::\n\n\n**Question:** What did you find? Keep this in mind for future Feature Engineering.\n\nThe findings show a decent increase in trips on memorial day compared to other Mondays.\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Question:** When are the peak hours? How do weekends differ from weekdays?\n\nWeekday peak hours are hour 7.5 and hour 17.5, weekend peak hours are a range between hour 10 and hour 20.\n\nWeekends do not have any sharp peaks presumably due to significant decrease in morning and evening commutes. Instead the later half of weekend days sees an increase in trips with fewer in the beginning of weekend days.\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,278 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 4,751 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,181 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 4,028 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 4,000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 3,946 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,914 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 3,899 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,856 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 3,767 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 3,629 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,552 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,516 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,213 </td>\n   <td style=\"text-align:right;\"> 39.93887 </td>\n   <td style=\"text-align:right;\"> -75.16663 </td>\n   <td style=\"text-align:right;\"> 3,332 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 3,320 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 3,264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 3,242 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,193 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 3,170 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 3,088 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nWe'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\n# Dealing with missing data\n\nWe need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n# Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q2 2025: April 1 - June 30\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-04-01\",\n  date_end = \"2025-06-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature     Precipitation       Wind_Speed    \n Min.   : 33.00   Min.   :0.00000   Min.   : 0.000  \n 1st Qu.: 57.00   1st Qu.:0.00000   1st Qu.: 5.000  \n Median : 65.00   Median :0.00000   Median : 8.000  \n Mean   : 64.69   Mean   :0.01121   Mean   : 8.103  \n 3rd Qu.: 72.00   3rd Qu.:0.00010   3rd Qu.:10.000  \n Max.   :100.00   Max.   :1.14000   Max.   :40.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q2 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 179107\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2177\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 548,604 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 179,107 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 369,497 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 548,604 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   : 33.0   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.: 57.0   1st Qu.:0.0000  \n Median : 0.0000   Median : 65.0   Median :0.0000  \n Mean   : 0.6055   Mean   : 64.7   Mean   :0.0111  \n 3rd Qu.: 1.0000   3rd Qu.: 72.0   3rd Qu.:0.0001  \n Max.   :27.0000   Max.   :100.0   Max.   :1.1400  \n                   NA's   :6048    NA's   :6048    \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be \\~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2), \n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 727,776 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on June 15, we need to predict demand for June 15–31. We have data from April 1 – June 14, but NOT from June 15–31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 23-26, test on weeks 14-23 (predicting past from future!)\n\n**Correct approach:** Train on weeks 14-23, test on weeks 23-26 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q2 has weeks 14–26 (Apr–Jun)\n# Train on weeks 14–22 (Apr 1 – early June)\n# Test on weeks 23–26 (rest of June)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week >= 14 & week < 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 23 & week <= 26) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week >= 14 & week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23 & week <= 26)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 494,514 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 220,365 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20180 to 20242 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20243 to 20269 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5883 -0.6612 -0.2097  0.2039 25.5514 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.6426952  0.0135882 -47.298 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0408056  0.0110267  -3.701             0.000215 ***\nas.factor(hour)2  -0.0637094  0.0110950  -5.742   0.0000000093534267 ***\nas.factor(hour)3  -0.1043859  0.0111732  -9.343 < 0.0000000000000002 ***\nas.factor(hour)4  -0.0854174  0.0111768  -7.642   0.0000000000000214 ***\nas.factor(hour)5   0.0140693  0.0110151   1.277             0.201505    \nas.factor(hour)6   0.2350758  0.0112781  20.844 < 0.0000000000000002 ***\nas.factor(hour)7   0.4887824  0.0110382  44.281 < 0.0000000000000002 ***\nas.factor(hour)8   0.8203572  0.0109917  74.634 < 0.0000000000000002 ***\nas.factor(hour)9   0.6052784  0.0109224  55.416 < 0.0000000000000002 ***\nas.factor(hour)10  0.4833367  0.0109423  44.171 < 0.0000000000000002 ***\nas.factor(hour)11  0.5266345  0.0107267  49.096 < 0.0000000000000002 ***\nas.factor(hour)12  0.5461066  0.0109059  50.075 < 0.0000000000000002 ***\nas.factor(hour)13  0.5459427  0.0107391  50.837 < 0.0000000000000002 ***\nas.factor(hour)14  0.5796075  0.0106800  54.270 < 0.0000000000000002 ***\nas.factor(hour)15  0.6895507  0.0107422  64.191 < 0.0000000000000002 ***\nas.factor(hour)16  0.8399549  0.0107905  77.842 < 0.0000000000000002 ***\nas.factor(hour)17  1.1006817  0.0107020 102.848 < 0.0000000000000002 ***\nas.factor(hour)18  0.8921751  0.0110968  80.399 < 0.0000000000000002 ***\nas.factor(hour)19  0.6187101  0.0108660  56.940 < 0.0000000000000002 ***\nas.factor(hour)20  0.3787883  0.0111030  34.116 < 0.0000000000000002 ***\nas.factor(hour)21  0.2408536  0.0110772  21.743 < 0.0000000000000002 ***\nas.factor(hour)22  0.1611046  0.0113413  14.205 < 0.0000000000000002 ***\nas.factor(hour)23  0.0800541  0.0108636   7.369   0.0000000000001721 ***\ndotw_simple2       0.0394716  0.0060987   6.472   0.0000000000967369 ***\ndotw_simple3      -0.0387619  0.0059569  -6.507   0.0000000000766954 ***\ndotw_simple4       0.0368225  0.0058552   6.289   0.0000000003200372 ***\ndotw_simple5      -0.0191931  0.0058073  -3.305             0.000950 ***\ndotw_simple6      -0.0396981  0.0060443  -6.568   0.0000000000511049 ***\ndotw_simple7      -0.0593577  0.0062299  -9.528 < 0.0000000000000002 ***\nTemperature        0.0136356  0.0001697  80.345 < 0.0000000000000002 ***\nPrecipitation     -0.0820133  0.0214920  -3.816             0.000136 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.103 on 494482 degrees of freedom\nMultiple R-squared:  0.1083,\tAdjusted R-squared:  0.1082 \nF-statistic:  1937 on 31 and 494482 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference in expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n-uesday and Thursday have positive coefficients (+0.039 and +0.037), meaning slightly higher demand than Monday. -Wednesday and Friday have negative coefficients (−0.039 and −0.019), indicating lower demand than Monday. -Q2 weekday behavior is more mixed than Q1 — likely reflecting decreased commuter uniformity in spring.\n\n**Weekend Pattern (Sat-Sun):**\n\nBoth weekend days have negative coefficients (−0.040 and −0.059) -This means fewer trips per station-hour than Monday, consistent with reduced weekday commuting\n\n**Hourly Interpretation**\n\nHour Coefficient Interpretation 0 (baseline) 0.000 trips/hour (midnight) 1 −0.041 slightly fewer than midnight ... 6 +0.235 morning activity starting 7 +0.489 morning rush building 8 +0.489 PEAK morning rush 9 +0.605 post-rush ... 17 +0.892 PEAK evening rush (5 PM!) 18 +0.619 evening declining ... 23 +0.080 late night minimal\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.7655  -0.4055  -0.1175   0.1064  20.1565 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.2511355  0.0115706 -21.705 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0131594  0.0093588  -1.406              0.15969    \nas.factor(hour)2  -0.0038870  0.0094196  -0.413              0.67986    \nas.factor(hour)3  -0.0298993  0.0094886  -3.151              0.00163 ** \nas.factor(hour)4  -0.0124325  0.0094990  -1.309              0.19060    \nas.factor(hour)5   0.0675634  0.0093629   7.216    0.000000000000536 ***\nas.factor(hour)6   0.2360275  0.0095891  24.614 < 0.0000000000000002 ***\nas.factor(hour)7   0.3773149  0.0093904  40.181 < 0.0000000000000002 ***\nas.factor(hour)8   0.5671605  0.0093632  60.574 < 0.0000000000000002 ***\nas.factor(hour)9   0.2384426  0.0093131  25.603 < 0.0000000000000002 ***\nas.factor(hour)10  0.1851136  0.0093129  19.877 < 0.0000000000000002 ***\nas.factor(hour)11  0.2443162  0.0091320  26.754 < 0.0000000000000002 ***\nas.factor(hour)12  0.2815768  0.0092792  30.345 < 0.0000000000000002 ***\nas.factor(hour)13  0.2947434  0.0091341  32.268 < 0.0000000000000002 ***\nas.factor(hour)14  0.3140117  0.0090855  34.562 < 0.0000000000000002 ***\nas.factor(hour)15  0.3934321  0.0091415  43.038 < 0.0000000000000002 ***\nas.factor(hour)16  0.4900066  0.0091922  53.307 < 0.0000000000000002 ***\nas.factor(hour)17  0.6744430  0.0091349  73.832 < 0.0000000000000002 ***\nas.factor(hour)18  0.3700542  0.0094939  38.978 < 0.0000000000000002 ***\nas.factor(hour)19  0.2047489  0.0092751  22.075 < 0.0000000000000002 ***\nas.factor(hour)20  0.0636594  0.0094662   6.725    0.000000000017589 ***\nas.factor(hour)21  0.0511361  0.0094204   5.428    0.000000056949989 ***\nas.factor(hour)22  0.0459849  0.0096319   4.774    0.000001804386998 ***\nas.factor(hour)23  0.0279605  0.0092200   3.033              0.00242 ** \ndotw_simple2       0.0157372  0.0051757   3.041              0.00236 ** \ndotw_simple3      -0.0343289  0.0050579  -6.787    0.000000000011446 ***\ndotw_simple4       0.0215345  0.0049689   4.334    0.000014655443825 ***\ndotw_simple5      -0.0123520  0.0049286  -2.506              0.01221 *  \ndotw_simple6      -0.0217358  0.0051311  -4.236    0.000022751610687 ***\ndotw_simple7      -0.0388840  0.0052889  -7.352    0.000000000000196 ***\nTemperature        0.0038829  0.0001461  26.578 < 0.0000000000000002 ***\nPrecipitation     -0.1091124  0.0182410  -5.982    0.000000002209391 ***\nlag1Hour           0.4200078  0.0013171 318.885 < 0.0000000000000002 ***\nlag3Hours          0.1215550  0.0013011  93.425 < 0.0000000000000002 ***\nlag1day            0.1303581  0.0011983 108.783 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9358 on 494479 degrees of freedom\nMultiple R-squared:  0.3578,\tAdjusted R-squared:  0.3578 \nF-statistic:  8104 on 34 and 494479 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n**Question:** Did adding lags improve R²? Why or why not?\n\nYes adding temporal lags greatly improved the model’s R². Model 1 had an R² of 0.108, meaning it explained only about 10.8% of the variation in hourly station-level trip counts. After adding lag variables, Model 2’s R² increased to 0.358, meaning it now explains 35.8% of the variation.\n\nThis improvement occurs because bike demand is temporally correlated. What happened at a station in the past hour, the past few hours, or the same hour yesterday can be very useful in predicting of what will happen next. These lag variables capture short-term daily usage patterns that day of the week or weather cannot explain.\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2175 -0.6608 -0.2560  0.4036 20.4657 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.4214563275  0.0364754779  11.555\nas.factor(hour)1          0.0502668639  0.0386686508   1.300\nas.factor(hour)2          0.0563469432  0.0412884048   1.365\nas.factor(hour)3         -0.0844099679  0.0525730268  -1.606\nas.factor(hour)4         -0.0488791828  0.0502419686  -0.973\nas.factor(hour)5          0.0275104664  0.0351233206   0.783\nas.factor(hour)6          0.3104649905  0.0305903192  10.149\nas.factor(hour)7          0.4137462039  0.0283369656  14.601\nas.factor(hour)8          0.6240419382  0.0273364062  22.828\nas.factor(hour)9          0.1245008093  0.0275060025   4.526\nas.factor(hour)10         0.1144654194  0.0278754891   4.106\nas.factor(hour)11         0.1445617964  0.0273324922   5.289\nas.factor(hour)12         0.2066648220  0.0274433635   7.531\nas.factor(hour)13         0.2425391713  0.0272432160   8.903\nas.factor(hour)14         0.1990225962  0.0269254814   7.392\nas.factor(hour)15         0.3115183565  0.0268084406  11.620\nas.factor(hour)16         0.4563089723  0.0267385494  17.066\nas.factor(hour)17         0.6930585998  0.0264449192  26.208\nas.factor(hour)18         0.2768284707  0.0269172514  10.284\nas.factor(hour)19         0.0911233969  0.0270639850   3.367\nas.factor(hour)20        -0.0247157229  0.0279925402  -0.883\nas.factor(hour)21        -0.0268175617  0.0287418109  -0.933\nas.factor(hour)22        -0.0150423629  0.0298726738  -0.504\nas.factor(hour)23        -0.0035088126  0.0304316654  -0.115\ndotw_simple2              0.0204091684  0.0116894619   1.746\ndotw_simple3             -0.0587324231  0.0118371322  -4.962\ndotw_simple4             -0.0085991237  0.0113426948  -0.758\ndotw_simple5             -0.0697512927  0.0111995049  -6.228\ndotw_simple6              0.0348851043  0.0119418747   2.921\ndotw_simple7              0.0055524868  0.0123456953   0.450\nTemperature               0.0077464671  0.0003522287  21.993\nPrecipitation            -0.3434418511  0.0361477360  -9.501\nlag1Hour                  0.3103941871  0.0021759549 142.647\nlag3Hours                 0.0878751818  0.0022378260  39.268\nlag1day                   0.1154785042  0.0021242651  54.362\nMed_Inc.x                 0.0000003349  0.0000001091   3.071\nPercent_Taking_Transit.y -0.0039827899  0.0004095445  -9.725\nPercent_White.y           0.0025037562  0.0002015520  12.422\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                      0.19362    \nas.factor(hour)2                      0.17234    \nas.factor(hour)3                      0.10837    \nas.factor(hour)4                      0.33062    \nas.factor(hour)5                      0.43348    \nas.factor(hour)6         < 0.0000000000000002 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9           0.0000060065765638 ***\nas.factor(hour)10          0.0000402233983358 ***\nas.factor(hour)11          0.0000001231467379 ***\nas.factor(hour)12          0.0000000000000508 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14          0.0000000000001458 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19                     0.00076 ***\nas.factor(hour)20                     0.37727    \nas.factor(hour)21                     0.35080    \nas.factor(hour)22                     0.61458    \nas.factor(hour)23                     0.90821    \ndotw_simple2                          0.08082 .  \ndotw_simple3               0.0000006994872069 ***\ndotw_simple4                          0.44838    \ndotw_simple5               0.0000000004734212 ***\ndotw_simple6                          0.00349 ** \ndotw_simple7                          0.65289    \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                             0.00213 ** \nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.209 on 155358 degrees of freedom\n  (339118 observations deleted due to missingness)\nMultiple R-squared:  0.2491,\tAdjusted R-squared:  0.2489 \nF-statistic:  1393 on 37 and 155358 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2719806 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.270657 \n```\n\n\n:::\n:::\n\n\n**What do station fixed effects capture?**\n\nCompared to earlier models, adding station fixed effects controls for, station-specific characteristics that make certain stations busier. Examples could be:\n\n-proximity to a university, job center, or tourist attractions -neighborhood density and land-use type -transit connectivity -bike share availability -socioeconomic patterns not fully captured by tract demographics -station upkeep\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2761162 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2747861 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Part 2 - Model Comparison to Q1 2025\n\nAcross all five models, Q2 2025 has higher MAE values than Q1 2025, especially once demographic and station fixed effects are added. Q2 MAE range: \\~0.62–0.90, Q1 MAE range: \\~0.50–0.73. Q2 has more volatile, ridership during spring weather (April–June). Demand is less predictable because of major seasonal events start of summer, festivals, recreational trips etc. campus activity at Penn/Drexel winding down at inconsistent times, and more weekend variability In contrast, Q1 demand is more stable, dominated by predictable cold-weather commuter traffic. Q1 (Winter) shows very strong AM and PM commuting peaks, low late-night and weekend activity. Q2 (Spring/Summer) shows stronger midday and evening activity, higher weekend ridership, greater spread in observed trips, weather variability that create sudden spikes/drops. This explains why the model in Q2 underpredicts during peaks and overpredicts during quiet hours, the ridership is more chaotic. Based on coefficient sizes and interpretability the most important predictors in Q2 are temporal lags - lag1Hour, lag3Hours, lag1day being the strongest predictors, and time weather, hourly effects - Q2 shows a broader spread of busy hours not just commute peaks, Temperature - more influential in Q2 than Q1, warm days increase ridership cold days in Q1 suppress it, precipitation - stronger negative effect in Q2 because rain ruins recreational trips. Q1 models perform better because winter ridership is more stable and predictable. Q2 errors are higher due to weather changed, recreational riders, tourism, and more irregular activity patterns. Lag variables and temperature become especially important in Q2, while demographic variables matter less.Temporal patterns broaden in Q2 making the system harder to predict.\n\n------------------------------------------------------------------------\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.62 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n**Question:** Which features gave us the biggest improvement?\n\nTemporal lags and Time weather\n\n------------------------------------------------------------------------\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nLet's use our best model (Model 2) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Question:** Where is the model performing well? Where is it struggling?\n\nModel 2 is very strong at predicting normal, low-trip periods, because temporal lag works well when demand is stable.\n\nThe model struggles during high-demand or irregular periods like weekday rush hours and weekends because those spikes require more features to predict accurately.\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n\n\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/spatial_errors-2.png){width=672}\n:::\n:::\n\n\n**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?\n\nYes the prediction errors show spatial clustering.\n\nThe model struggles most in the area around Center City, University City, and the surrounding high-activity neighborhoods.\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Untitled_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?\n\nPrediction errors vary systematically with neighborhood demographics. Stations in higher-income and majority White areas show larger errors, while stations in transit-dependent, lower-income, and more racially diverse neighborhoods have lower errors.\n\nThe model does not disadvantage historically undeserved communities, instead its limitations appear in wealthier areas where rider behavior is more variable.\n\n# Full Error Analysis\n\n*Spatial patterns*\n\nThe errors cluster in and around the busiest, mixed-use parts of the city: Center City, University City, Northern Liberties/Fishtown, and parts of South Philly near Graduate Hospital. These are stations with very high and volatile demand, where the model tends to under-predict during spikes and sometimes over-predict during quieter hours. In contrast, peripheral and more residential areas have lower errors, because demand is smaller and more stable. A reasonable hypothesis is that the model is missing features that explain things like special events, tourism, nightlife, university proximity, detailed land use, and station capacity. Those factors matter most exactly where the errors are largest.\n\n*Temporal patterns*\n\nErrors are lowest overnight and during very low demand periods, when most stations have 0–2 trips and the model can match them easily. Errors are highest during AM and PM rush hours and some mid-day/evening periods, especially on weekdays, where the model under-predicts high trip counts i.e., 10–20+ trips per station hour. Weekends also show more noise and irregular behavior, making them harder to capture than regular weekday commuting. Within Q2, warmer, nicer days tend to produce larger errors because demand spikes sharply, while cooler or rainy days are easier to predict since overall usage drops and becomes more consistent.\n\n*Demographic patterns*\n\nWhen errors are related to census characteristics, they are larger in higher-income, majority-White neighborhoods and smaller in more transit-dependent, lower-income, and racially diverse areas. That means the model does better in communities that are more likely to rely on bikes and transit for everyday mobility, and it struggles more in wealthier areas where ridership is more discretionary, recreational, or event-driven. This means, certain communities are harder to predict but they tend to be higher-income, White neighborhoods, not historically underserved ones. The equity implication is that the model does not appear to worsen access for vulnerable communities based on prediction error alone. Still, if Indego allocates bikes purely based on predicted demand volume, resources could drift toward high-use central/wealthy areas unless equity safeguards like minimum service levels and equity-weighted rules are put in place.\n\n------------------------------------------------------------------------\n\n# Part 3: Feature Engineering & model improvement\n\n# Create Features\n\n# Rolling 7-Day Average & Perfect Weather\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rolling 7-day average using lag()\ntrain <- train %>%\n  group_by(start_station) %>%\n  arrange(interval60) %>%\n  mutate(\n    lag24  = lag(Trip_Count, 24),\n    lag48  = lag(Trip_Count, 48),\n    lag72  = lag(Trip_Count, 72),\n    lag96  = lag(Trip_Count, 96),\n    lag120 = lag(Trip_Count, 120),\n    lag144 = lag(Trip_Count, 144),\n    lag168 = lag(Trip_Count, 168),\n    roll7day = rowMeans(\n      cbind(lag24, lag48, lag72, lag96, lag120, lag144, lag168),\n      na.rm = TRUE\n    )\n  ) %>%\n  ungroup()\n\ntest <- test %>%\n  group_by(start_station) %>%\n  arrange(interval60) %>%\n  mutate(\n    lag24  = lag(Trip_Count, 24),\n    lag48  = lag(Trip_Count, 48),\n    lag72  = lag(Trip_Count, 72),\n    lag96  = lag(Trip_Count, 96),\n    lag120 = lag(Trip_Count, 120),\n    lag144 = lag(Trip_Count, 144),\n    lag168 = lag(Trip_Count, 168),\n    roll7day = rowMeans(\n      cbind(lag24, lag48, lag72, lag96, lag120, lag144, lag168),\n      na.rm = TRUE\n    )\n  ) %>%\n  ungroup()\n\n# Perfect biking weather indicator - 60–75°F and no rain\ntrain <- train %>%\n  mutate(\n    perfect_weather = ifelse(\n      Temperature >= 60 & Temperature <= 75 & Precipitation == 0,\n      1, 0\n    )\n  )\n\ntest <- test %>%\n  mutate(\n    perfect_weather = ifelse(\n      Temperature >= 60 & Temperature <= 75 & Precipitation == 0,\n      1, 0\n    )\n  )\n```\n:::\n\n\n# Fit Model with New Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2_improved <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    roll7day + perfect_weather,\n  data = train\n)\n\nsummary(model2_improved)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + roll7day + \n    perfect_weather, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6021  -0.4111  -0.1132   0.1423  20.7183 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.3131338  0.0118754 -26.368 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0195425  0.0093448  -2.091             0.036505 *  \nas.factor(hour)2  -0.0064578  0.0094048  -0.687             0.492299    \nas.factor(hour)3  -0.0351382  0.0094743  -3.709             0.000208 ***\nas.factor(hour)4  -0.0180424  0.0094847  -1.902             0.057138 .  \nas.factor(hour)5   0.0571895  0.0093475   6.118    0.000000000947348 ***\nas.factor(hour)6   0.2253291  0.0095858  23.506 < 0.0000000000000002 ***\nas.factor(hour)7   0.3690834  0.0093754  39.367 < 0.0000000000000002 ***\nas.factor(hour)8   0.5709139  0.0093639  60.970 < 0.0000000000000002 ***\nas.factor(hour)9   0.2456017  0.0093061  26.392 < 0.0000000000000002 ***\nas.factor(hour)10  0.1916175  0.0093100  20.582 < 0.0000000000000002 ***\nas.factor(hour)11  0.2486189  0.0091260  27.243 < 0.0000000000000002 ***\nas.factor(hour)12  0.2718628  0.0092671  29.336 < 0.0000000000000002 ***\nas.factor(hour)13  0.2819482  0.0091178  30.923 < 0.0000000000000002 ***\nas.factor(hour)14  0.3123198  0.0090726  34.424 < 0.0000000000000002 ***\nas.factor(hour)15  0.4044841  0.0091373  44.267 < 0.0000000000000002 ***\nas.factor(hour)16  0.5062081  0.0091899  55.083 < 0.0000000000000002 ***\nas.factor(hour)17  0.7080986  0.0091630  77.278 < 0.0000000000000002 ***\nas.factor(hour)18  0.4159141  0.0095387  43.603 < 0.0000000000000002 ***\nas.factor(hour)19  0.2475441  0.0093222  26.554 < 0.0000000000000002 ***\nas.factor(hour)20  0.1026710  0.0095122  10.794 < 0.0000000000000002 ***\nas.factor(hour)21  0.0623794  0.0094407   6.608    0.000000000039117 ***\nas.factor(hour)22  0.0540272  0.0096241   5.614    0.000000019808931 ***\nas.factor(hour)23  0.0318131  0.0092036   3.457             0.000547 ***\ndotw_simple2       0.0274932  0.0051733   5.314    0.000000107056200 ***\ndotw_simple3      -0.0276715  0.0051764  -5.346    0.000000090081677 ***\ndotw_simple4       0.0315167  0.0049707   6.340    0.000000000229244 ***\ndotw_simple5       0.0058270  0.0049107   1.187             0.235389    \ndotw_simple6      -0.0033395  0.0051392  -0.650             0.515818    \ndotw_simple7      -0.0311661  0.0052540  -5.932    0.000000002997721 ***\nTemperature        0.0033392  0.0001659  20.127 < 0.0000000000000002 ***\nPrecipitation     -0.0852136  0.0183261  -4.650    0.000003322576884 ***\nlag1Hour           0.4028298  0.0013317 302.493 < 0.0000000000000002 ***\nlag3Hours          0.1037009  0.0013154  78.835 < 0.0000000000000002 ***\nlag1day            0.0650888  0.0013929  46.728 < 0.0000000000000002 ***\nroll7day           0.2181330  0.0024797  87.969 < 0.0000000000000002 ***\nperfect_weather    0.0235454  0.0032741   7.191    0.000000000000642 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9284 on 488501 degrees of freedom\n  (5976 observations deleted due to missingness)\nMultiple R-squared:  0.3683,\tAdjusted R-squared:  0.3683 \nF-statistic:  7913 on 36 and 488501 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n# Predictions and MAE for Improved Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    pred2_improved = predict(model2_improved, newdata = test)\n  )\n\nmae_improved <- mean(abs(test$Trip_Count - test$pred2_improved), na.rm = TRUE)\nmae_improved\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6120655\n```\n\n\n:::\n:::\n\n\nTo improve Model 2, I added two features targeted at the specific error patterns in Q2. These variables were selected because they directly addressed the two biggest error sources observed in Q2, high variability in warm-weather ridership, and medium-term temporal patterns that hourly lags couldn't capture. The improved model reduced MAE from 0.62 to 0.61, with the greatest gains during midday and evening hours where the base model under predicted most. Although the improvement is small, it shows that targeted temporal and weather-based features can make the model more responsive to seasonal changes in Q2.\n\n------------------------------------------------------------------------\n\n# Part 4: Critical Reflection\n\nWith an MAE of 0.61, the improved Model 2 is accurate enough to support planning and routine operational decision-making, but not precise enough to drive highly time-sensitive rebalancing on its own. Errors are especially significant during peak hours and warm-weather surges, when even small underpredictions can create shortages or empty docks. Because of this, the model should be deployed as a forecasting and scheduling aid, useful for anticipating patterns several hours or days ahead, setting baseline staffing levels, and identifying stations that consistently trend above or below expectations. A realistic deployment would involve using the model for scheduled planning such as setting daily rebalancing priorities or anticipating high-demand periods—while relying on Indego’s existing operational rules to handle short-term fluctuations, as the model can potentialy improve foresight but doesnt over promise precision.\n\nEquity analysis shows that prediction errors are higher in higher-income, majority-White areas and lower in lower-income, transit-dependent, and racially diverse neighborhoods, meaning the model does not disproportionately disadvantage historically undeserved communities. Still, the system could worsen disparities if Indego relies too heavily on predicted demand volume rather than service goals. Wealthier neighborhoods often generate more rides, which could pull resources toward them if decisions are not equity-regulated. To avoid this, Indego should incorporate equity safeguards, such as ensuring minimum service levels at historically underserved stations and weighting rebalancing rules to prevent resource drift. The model also has notable limitations: it lacks information on special events, university features, tourism, and temporary infrastructure changes, and it assumes station-level patterns remain stable across weeks. With more time and data, integrating scheduled event calendars, land-use context, and more flexible modeling approaches would help the system capture these missing patterns and better support both operational reliability and equitable service distribution.\n\n------------------------------------------------------------------------\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}