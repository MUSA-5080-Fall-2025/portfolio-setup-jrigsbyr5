---
title: "Space-Time Prediction of Bike Share Demand: Philadelphia Indego"
author: "Joshua Rigsby"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
```

# Introduction

## The Rebalancing Challenge in Philadelphia

Philadelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**. 

Imagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have:
- 200 stations across Philadelphia
- Limited trucks and staff for moving bikes
- 2-3 hours before morning rush hour demand peaks
- **The question:** Which stations will run out of bikes by 8:30 AM?

This lab will build predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.

## Objectives

1. **Understand panel data structure** for space-time analysis
2. **Create temporal lag variables** to capture demand persistence
3. **Build multiple predictive models** with increasing complexity
4. **Validate models temporally** (train on past, test on future)
5. **Analyze prediction errors** in both space and time
6. **Engineer new features** based on error patterns
7. **Critically evaluate** when prediction errors matter most

---

# Part 1

# Setup

## Load Libraries

```{r load_libraries}
# Core tidyverse
library(tidyverse)
library(lubridate)

# Spatial data
library(sf)
library(tigris)

# Census data
library(tidycensus)

# Weather data
library(riem)  # For Philadelphia weather from ASOS stations

# Visualization
library(viridis)
library(gridExtra)
library(knitr)
library(kableExtra)

# here!
library(here)
# Get rid of scientific notation. We gotta look good!
options(scipen = 999)
```

## Define Themes

```{r themes}
plotTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title = element_text(size = 11, face = "bold"),
  panel.background = element_blank(),
  panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
  panel.grid.minor = element_blank(),
  axis.ticks = element_blank(),
  legend.position = "right"
)

mapTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.line = element_blank(),
  axis.text = element_blank(),
  axis.ticks = element_blank(),
  axis.title = element_blank(),
  panel.background = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.position = "right",
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(0.2, "cm")
)

palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
```

## Set Census API Key

```{r census_key, eval=FALSE}
Sys.setenv(CENSUS_API_KEY = "cb0a6c25010d92d0c374f6a32aa1ea3959229e34")
```

```{r census_key_hidden, include=FALSE}
# Hidden key for rendering
Sys.setenv(CENSUS_API_KEY = "cb0a6c25010d92d0c374f6a32aa1ea3959229e34")
```

---

# Data Import & Preparation

## Load Indego Trip Data (Q2 2025)

```{r load_indego}
# Read Q2 2025 data
indego <- read_csv("indego-trips-2025-q2.csv")

# Quick look at the data
glimpse(indego)
```

## Examine the Data Structure

```{r explore_data}
# How many trips?
cat("Total trips in Q2 2025:", nrow(indego), "\n")

# Date range
cat("Date range:", 
    min(mdy_hm(indego$start_time)), "to", 
    max(mdy_hm(indego$start_time)), "\n")

# How many unique stations?
cat("Unique start stations:", length(unique(indego$start_station)), "\n")

# Trip types
table(indego$trip_route_category)

# Passholder types
table(indego$passholder_type)

# Bike types
table(indego$bike_type)
```

## Create Time Bins

We need to aggregate trips into hourly intervals for our panel data structure.

```{r create_time_bins}
indego <- indego %>%
  mutate(
    # Parse datetime
    start_datetime = mdy_hm(start_time),
    end_datetime = mdy_hm(end_time),
    
    # Create hourly bins
    interval60 = floor_date(start_datetime, unit = "hour"),
    
    # Extract time features
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    
    # Create useful indicators
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )

# Look at temporal features
head(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))
```

---

# Exploratory Analysis

## Trips Over Time

```{r trips_over_time}
# Daily trip counts
daily_trips <- indego %>%
  group_by(date) %>%
  summarize(trips = n())

ggplot(daily_trips, aes(x = date, y = trips)) +
  geom_line(color = "#3182bd", linewidth = 1) +
  geom_smooth(se = FALSE, color = "red", linetype = "dashed") +
  labs(
    title = "Indego Daily Ridership - Q2 2025",
    subtitle = "Winter demand patterns in Philadelphia",
    x = "Date",
    y = "Daily Trips",
    caption = "Source: Indego bike share"
  ) +
  plotTheme
```

**Question:** What patterns do you see? How does ridership change over time?

The Patterns Radically shift frequently between two extremes. 

It seems the demand changes by time of day or portion of the week or month. There seems to be a low point mid way through each month.



Let's investigate a comparison between memorial day the 26th of May 2025  to all other Mondays in the data.

```{r}
# Memorial Day date for Q2 2025
holiday_date <- "2025-05-26"

# Trips on Memorial Day
holiday_trips <- daily_trips %>%
  filter(date == holiday_date)

# Typical Monday behavior
typical_monday <- indego %>%
  filter(dotw == "Mon", date != holiday_date) %>%
  group_by(date) %>%
  summarize(trips = n()) %>%
  summarize(avg_monday_trips = mean(trips))

# Comparison
print(holiday_trips)
print(typical_monday)


```
**Question:** What did you find? Keep this in mind for future Feature Engineering.

The findings show a decent increase in trips on memorial day compared to other Mondays.


## Hourly Patterns

```{r hourly_patterns}
# Average trips by hour and day type
hourly_patterns <- indego %>%
  group_by(hour, weekend) %>%
  summarize(avg_trips = n() / n_distinct(date)) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Average Hourly Ridership Patterns",
    subtitle = "Clear commute patterns on weekdays",
    x = "Hour of Day",
    y = "Average Trips per Hour",
    color = "Day Type"
  ) +
  plotTheme
```

**Question:** When are the peak hours? How do weekends differ from weekdays?

Weekday peak hours are hour 7.5 and hour 17.5, weekend peak hours are a range between hour 10 and hour 20.

Weekends do not have any sharp peaks presumably due to significant decrease in morning and evening commutes. Instead the later half of weekend days sees an increase in trips with fewer in the beginning of weekend days.

## Top Stations

```{r top_stations}
# Most popular origin stations
top_stations <- indego %>%
  count(start_station, start_lat, start_lon, name = "trips") %>%
  arrange(desc(trips)) %>%
  head(20)

kable(top_stations, 
      caption = "Top 20 Indego Stations by Trip Origins",
      format.args = list(big.mark = ",")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

---

# Get Philadelphia Spatial Context

## Load Philadelphia Census Data

We'll get census tract data to add demographic context to our stations.

```{r load_census}
# Get Philadelphia census tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    "B01003_001",  # Total population
    "B19013_001",  # Median household income
    "B08301_001",  # Total commuters
    "B08301_010",  # Commute by transit
    "B02001_002",  # White alone
    "B25077_001"   # Median home value
  ),
  state = "PA",
  county = "Philadelphia",
  year = 2022,
  geometry = TRUE,
  output = "wide"
) %>%
  rename(
    Total_Pop = B01003_001E,
    Med_Inc = B19013_001E,
    Total_Commuters = B08301_001E,
    Transit_Commuters = B08301_010E,
    White_Pop = B02001_002E,
    Med_Home_Value = B25077_001E
  ) %>%
  mutate(
    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,
    Percent_White = (White_Pop / Total_Pop) * 100
  ) %>%
  st_transform(crs = 4326)  # WGS84 for lat/lon matching

# Check the data
glimpse(philly_census)
```

## Map Philadelphia Context

```{r map_philly}
# Map median income
ggplot() +
  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract",
    subtitle = "Context for understanding bike share demand patterns"
  ) +
  # Stations 
  geom_point(
    data = indego,
    aes(x = start_lon, y = start_lat),
    color = "red", size = 0.25, alpha = 0.6
  ) +
  mapTheme
```

## Join Census Data to Stations

We'll spatially join census characteristics to each bike station.

```{r join_census_to_stations}
# Create sf object for stations
stations_sf <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  st_as_sf(coords = c("start_lon", "start_lat"), crs = 4326)

# Spatial join to get census tract for each station
stations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%
  st_drop_geometry()

# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.

stations_for_map <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

# Add back to trip data
indego_census <- indego %>%
  left_join(
    stations_census %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )


# Prepare data for visualization
stations_for_map <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

# Create the map showing problem stations
ggplot() +
  geom_sf(data = philly_census, aes(fill = Med_Inc), color = "white", size = 0.1) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar,
    na.value = "grey90"
  ) +
  # Stations with census data (small grey dots)
  geom_point(
    data = stations_for_map %>% filter(has_census),
    aes(x = start_lon, y = start_lat),
    color = "grey30", size = 1, alpha = 0.6
  ) +
  # Stations WITHOUT census data (red X marks the spot)
  geom_point(
    data = stations_for_map %>% filter(!has_census),
    aes(x = start_lon, y = start_lat),
    color = "red", size = 1, shape = 4, stroke = 1.5
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract",
    subtitle = "Indego stations shown (RED = no census data match)",
    caption = "Red X marks indicate stations that didn't join to census tracts"
  ) +
  mapTheme



```

# Dealing with missing data

We need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..


```{r}
# Identify which stations to keep
valid_stations <- stations_census %>%
  filter(!is.na(Med_Inc)) %>%
  pull(start_station)

# Filter trip data to valid stations only
indego_census <- indego %>%
  filter(start_station %in% valid_stations) %>%
  left_join(
    stations_census %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )

```


# Get Weather Data

Weather significantly affects bike share demand! Let's get hourly weather for Philadelphia.

```{r get_weather}
# Get weather from Philadelphia International Airport (KPHL)
# This covers Q2 2025: April 1 - June 30
weather_data <- riem_measures(
  station = "PHL",  # Philadelphia International Airport
  date_start = "2025-04-01",
  date_end = "2025-06-30"
)

# Process weather data
weather_processed <- weather_data %>%
  mutate(
    interval60 = floor_date(valid, unit = "hour"),
    Temperature = tmpf,  # Temperature in Fahrenheit
    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches
    Wind_Speed = sknt  # Wind speed in knots
  ) %>%
  select(interval60, Temperature, Precipitation, Wind_Speed) %>%
  distinct()

# Check for missing hours and interpolate if needed
weather_complete <- weather_processed %>%
  complete(interval60 = seq(min(interval60), max(interval60), by = "hour")) %>%
  fill(Temperature, Precipitation, Wind_Speed, .direction = "down")

# Look at the weather
summary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))
```

## Visualize Weather Patterns

Who is ready for a Philly winter?!

```{r visualize_weather}
ggplot(weather_complete, aes(x = interval60, y = Temperature)) +
  geom_line(color = "#3182bd", alpha = 0.7) +
  geom_smooth(se = FALSE, color = "red") +
  labs(
    title = "Philadelphia Temperature - Q2 2025",
    subtitle = "Winter to early spring transition",
    x = "Date",
    y = "Temperature (°F)"
  ) +
  plotTheme
```

---

# Create Space-Time Panel

## Aggregate Trips to Station-Hour Level

```{r aggregate_trips}
# Count trips by station-hour
trips_panel <- indego_census %>%
  group_by(interval60, start_station, start_lat, start_lon,
           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%
  summarize(Trip_Count = n()) %>%
  ungroup()

# How many station-hour observations?
nrow(trips_panel)

# How many unique stations?
length(unique(trips_panel$start_station))

# How many unique hours?
length(unique(trips_panel$interval60))
```

## Create Complete Panel Structure

Not every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).

```{r complete_panel}
# Calculate expected panel size
n_stations <- length(unique(trips_panel$start_station))
n_hours <- length(unique(trips_panel$interval60))
expected_rows <- n_stations * n_hours

cat("Expected panel rows:", format(expected_rows, big.mark = ","), "\n")
cat("Current rows:", format(nrow(trips_panel), big.mark = ","), "\n")
cat("Missing rows:", format(expected_rows - nrow(trips_panel), big.mark = ","), "\n")

# Create complete panel
study_panel <- expand.grid(
  interval60 = unique(trips_panel$interval60),
  start_station = unique(trips_panel$start_station)
) %>%
  # Join trip counts
  left_join(trips_panel, by = c("interval60", "start_station")) %>%
  # Replace NA trip counts with 0
  mutate(Trip_Count = replace_na(Trip_Count, 0))

# Fill in station attributes (they're the same for all hours)
station_attributes <- trips_panel %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(start_lat),
    start_lon = first(start_lon),
    Med_Inc = first(Med_Inc),
    Percent_Taking_Transit = first(Percent_Taking_Transit),
    Percent_White = first(Percent_White),
    Total_Pop = first(Total_Pop)
  )

study_panel <- study_panel %>%
  left_join(station_attributes, by = "start_station")

# Verify we have complete panel
cat("Complete panel rows:", format(nrow(study_panel), big.mark = ","), "\n")
```

## Add Time Features

```{r add_time_features}
study_panel <- study_panel %>%
  mutate(
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

## Join Weather Data

```{r join_weather}
study_panel <- study_panel %>%
  left_join(weather_complete, by = "interval60")

# Check for missing values
summary(study_panel %>% select(Trip_Count, Temperature, Precipitation))
```

---

# Create Temporal Lag Variables

The key innovation for space-time prediction: **past demand predicts future demand**.

## Why Lags?

If there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.

```{r create_lags}
# Sort by station and time
study_panel <- study_panel %>%
  arrange(start_station, interval60)

# Create lag variables WITHIN each station
study_panel <- study_panel %>%
  group_by(start_station) %>%
  mutate(
    lag1Hour = lag(Trip_Count, 1),
    lag2Hours = lag(Trip_Count, 2), 
    lag3Hours = lag(Trip_Count, 3),
    lag12Hours = lag(Trip_Count, 12),
    lag1day = lag(Trip_Count, 24)
  ) %>%
  ungroup()

# Remove rows with NA lags (first 24 hours for each station)
study_panel_complete <- study_panel %>%
  filter(!is.na(lag1day))

cat("Rows after removing NA lags:", format(nrow(study_panel_complete), big.mark = ","), "\n")
```

## Visualize Lag Correlations

```{r lag_correlations}
# Sample one station to visualize
example_station <- study_panel_complete %>%
  filter(start_station == first(start_station)) %>%
  head(168)  # One week

# Plot actual vs lagged demand
ggplot(example_station, aes(x = interval60)) +
  geom_line(aes(y = Trip_Count, color = "Current"), linewidth = 1) +
  geom_line(aes(y = lag1Hour, color = "1 Hour Ago"), linewidth = 1, alpha = 0.7) +
  geom_line(aes(y = lag1day, color = "24 Hours Ago"), linewidth = 1, alpha = 0.7) +
  scale_color_manual(values = c(
    "Current" = "#08519c",
    "1 Hour Ago" = "#3182bd",
    "24 Hours Ago" = "#6baed6"
  )) +
  labs(
    title = "Temporal Lag Patterns at One Station",
    subtitle = "Past demand predicts future demand",
    x = "Date-Time",
    y = "Trip Count",
    color = "Time Period"
  ) +
  plotTheme
```

---

# Temporal Train/Test Split

**CRITICAL:** We must train on PAST data and test on FUTURE data!

## Why Temporal Validation Matters

In real operations, at 6:00 AM on June 15, we need to predict demand for June 15–31. We have data from April 1 – June 14, but NOT from June 15–31 (it hasn't happened yet!).

**Wrong approach:** Train on weeks 23-26, test on weeks 14-23 (predicting past from future!)

**Correct approach:** Train on weeks 14-23, test on weeks 23-26 (predicting future from past)

```{r temporal_split}

# Split by week
# Q2 has weeks 14–26 (Apr–Jun)
# Train on weeks 14–22 (Apr 1 – early June)
# Test on weeks 23–26 (rest of June)

# Which stations have trips in BOTH early and late periods?
early_stations <- study_panel_complete %>%
  filter(week >= 14 & week < 23) %>%
  filter(Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

late_stations <- study_panel_complete %>%
  filter(week >= 23 & week <= 26) %>%
  filter(Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

# Keep only stations that appear in BOTH periods
common_stations <- intersect(early_stations, late_stations)

# Filter panel to only common stations
study_panel_complete <- study_panel_complete %>%
  filter(start_station %in% common_stations)

# NOW create train/test split
train <- study_panel_complete %>%
  filter(week >= 14 & week < 23)

test <- study_panel_complete %>%
  filter(week >= 23 & week <= 26)

cat("Training observations:", format(nrow(train), big.mark = ","), "\n")
cat("Testing observations:", format(nrow(test), big.mark = ","), "\n")
cat("Training date range:", min(train$date), "to", max(train$date), "\n")
cat("Testing date range:", min(test$date), "to", max(test$date), "\n")

```

---

# Build Predictive Models

We'll build 5 models with increasing complexity to see what improves predictions.

## Model 1: Baseline (Time + Weather)

```{r model1}

# Create day of week factor with treatment (dummy) coding
train <- train %>%
  mutate(dotw_simple = factor(dotw, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")))

# Set contrasts to treatment coding (dummy variables)
contrasts(train$dotw_simple) <- contr.treatment(7)

# Now run the model
model1 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,
  data = train
)

summary(model1)
```

The model uses Monday as the baseline. Each coefficient represents the difference 
in expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..

**Weekday Pattern (Tue-Fri):**

-Tuesday and Thursday have positive coefficients (+0.039 and +0.037), meaning slightly higher demand than Monday.
-Wednesday and Friday have negative coefficients (−0.039 and −0.019), indicating lower demand than Monday.
-Q2 weekday behavior is more mixed than Q1 — likely reflecting decreased commuter uniformity in spring.
**Weekend Pattern (Sat-Sun):**

-Both weekend days have negative coefficients (−0.040 and −0.059)
-This means fewer trips per station-hour than Monday, consistent with reduced weekday commuting

**Hourly Interpretation**

Hour   Coefficient   Interpretation
0      (baseline)    0.000 trips/hour (midnight)
1      −0.041       slightly fewer than midnight
...
6      +0.235       morning activity starting
7      +0.489       morning rush building
8      +0.489       PEAK morning rush
9      +0.605       post-rush
...
17     +0.892      PEAK evening rush (5 PM!)
18     +0.619       evening declining
...
23     +0.080       late night minimal


## Model 2: Add Temporal Lags

```{r model2}
model2 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day,
  data = train
)

summary(model2)
```

**Question:** Did adding lags improve R²? Why or why not?

Yes adding temporal lags greatly improved the model’s R². Model 1 had an R² of 0.108, meaning it explained only about 10.8% of the variation in hourly station-level trip counts.
After adding lag variables, Model 2’s R² increased to 0.358, meaning it now explains 35.8% of the variation.

This improvement occurs because bike demand is temporally correlated. What happened at a station in the past hour, the past few hours, or the same hour yesterday can be very useful in predicting of what will happen next. These lag variables capture short-term daily usage patterns that day of the week or weather cannot explain.


## Model 3: Add Demographics

```{r model3}
model3 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,
  data = train
)

summary(model3)
```

## Model 4: Add Station Fixed Effects

```{r model4}
model4 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +
    as.factor(start_station),
  data = train
)

# Summary too long with all station dummies, just show key metrics
cat("Model 4 R-squared:", summary(model4)$r.squared, "\n")
cat("Model 4 Adj R-squared:", summary(model4)$adj.r.squared, "\n")
```

**What do station fixed effects capture?** 

Compared to earlier models, adding station fixed effects controls for, station-specific characteristics that make certain stations busier. Examples could be:
-proximity to a university, job center, or tourist attractions
-neighborhood density and land-use type
-transit connectivity
-bike share availability
-socioeconomic patterns not fully captured by tract demographics
-station upkeep

## Model 5: Add Rush Hour Interaction

```{r model5}
model5 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +
    as.factor(start_station) +
    rush_hour * weekend,  # Rush hour effects different on weekends
  data = train
)

cat("Model 5 R-squared:", summary(model5)$r.squared, "\n")
cat("Model 5 Adj R-squared:", summary(model5)$adj.r.squared, "\n")
```

---

# Part 2 - Model Comparison to Q1 2025

Across all five models, Q2 2025 has higher MAE values than Q1 2025, especially once demographic and station fixed effects are added. Q2 MAE range: ~0.62–0.90, Q1 MAE range: ~0.50–0.73. 
Q2 has more volatile, ridership during spring weather (April–June). Demand is less predictable because of major seasonal events start of summer, festivals, recreational trips etc. campus activity at Penn/Drexel winding down at inconsistent times, and more weekend variability
In contrast, Q1 demand is more stable, dominated by predictable cold-weather commuter traffic.
Q1 (Winter) shows very strong AM and PM commuting peaks, low late-night and weekend activity.
Q2 (Spring/Summer) shows stronger midday and evening activity, higher weekend ridership,
greater spread in observed trips, weather variability that create sudden spikes/drops.
This explains why the model in Q2 underpredicts during peaks and overpredicts during quiet hours, the ridership is more chaotic. Based on coefficient sizes and interpretability the most important predictors in Q2 are temporal lags - lag1Hour, lag3Hours, lag1day being  the strongest predictors, and time weather, hourly effects - Q2 shows a broader spread of busy hours not just commute peaks, Temperature - more influential in Q2 than Q1, warm days increase ridership cold days in Q1 suppress it, precipitation - stronger negative effect in Q2 because rain ruins recreational trips. Q1 models perform better because winter ridership is more stable and predictable. Q2 errors are higher due to weather changed, recreational riders, tourism, and more irregular activity patterns. Lag variables and temperature become especially important in Q2, while demographic variables matter less.Temporal patterns broaden in Q2 making the system harder to predict.

---
# Model Evaluation

## Calculate Predictions and MAE

```{r calculate_mae}
# Get predictions on test set

# Create day of week factor with treatment (dummy) coding
test <- test %>%
  mutate(dotw_simple = factor(dotw, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")))

# Set contrasts to treatment coding (dummy variables)
contrasts(test$dotw_simple) <- contr.treatment(7)

test <- test %>%
  mutate(
    pred1 = predict(model1, newdata = test),
    pred2 = predict(model2, newdata = test),
    pred3 = predict(model3, newdata = test),
    pred4 = predict(model4, newdata = test),
    pred5 = predict(model5, newdata = test)
  )

# Calculate MAE for each model
mae_results <- data.frame(
  Model = c(
    "1. Time + Weather",
    "2. + Temporal Lags",
    "3. + Demographics",
    "4. + Station FE",
    "5. + Rush Hour Interaction"
  ),
  MAE = c(
    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)
  )
)

kable(mae_results, 
      digits = 2,
      caption = "Mean Absolute Error by Model (Test Set)",
      col.names = c("Model", "MAE (trips)")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Visualize Model Comparison

```{r compare_models}
ggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +
  geom_col(fill = "#3182bd", alpha = 0.8) +
  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +
  labs(
    title = "Model Performance Comparison",
    subtitle = "Lower MAE = Better Predictions",
    x = "Model",
    y = "Mean Absolute Error (trips)"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Question:** Which features gave us the biggest improvement?

Temporal lags and Time weather

---

# Space-Time Error Analysis

## Observed vs. Predicted

Let's use our best model (Model 2) for error analysis.

```{r obs_vs_pred}
test <- test %>%
  mutate(
    error = Trip_Count - pred2,
    abs_error = abs(error),
    time_of_day = case_when(
      hour < 7 ~ "Overnight",
      hour >= 7 & hour < 10 ~ "AM Rush",
      hour >= 10 & hour < 15 ~ "Mid-Day",
      hour >= 15 & hour <= 18 ~ "PM Rush",
      hour > 18 ~ "Evening"
    )
  )

# Scatter plot by time and day type
ggplot(test, aes(x = Trip_Count, y = pred2)) +
  geom_point(alpha = 0.2, color = "#3182bd") +
  geom_abline(slope = 1, intercept = 0, color = "red", linewidth = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  facet_grid(weekend ~ time_of_day) +
  labs(
    title = "Observed vs. Predicted Bike Trips",
    subtitle = "Model 2 performance by time period",
    x = "Observed Trips",
    y = "Predicted Trips",
    caption = "Red line = perfect predictions; Green line = actual model fit"
  ) +
  plotTheme
```

**Question:** Where is the model performing well? Where is it struggling?

Model 2 is very strong at predicting normal, low-trip periods, because temporal lag works well when demand is stable.

The model struggles during high-demand or irregular periods like weekday rush hours and weekends because those spikes require more features to predict accurately.

## Spatial Error Patterns

Are prediction errors clustered in certain parts of Philadelphia?

```{r spatial_errors}
# Calculate MAE by station
station_errors <- test %>%
  group_by(start_station, start_lat.x, start_lon.y) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(start_lat.x), !is.na(start_lon.y))

## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)

# Calculate station errors
station_errors <- test %>%
  filter(!is.na(pred2)) %>%
  group_by(start_station, start_lat.x, start_lon.y) %>%
  summarize(
    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(start_lat.x), !is.na(start_lon.y))

# Map 1: Prediction Errors
p1 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors,
    aes(x = start_lon, y = start_lat, color = MAE),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "plasma",
    name = "MAE\n(trips)",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks
    labels = c("0.5", "1.0", "1.5")
  ) +
  labs(title = "Prediction Errors",
       subtitle = "Higher in Center City") +
  mapTheme +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10)
  ) +
  guides(color = guide_colorbar(
    barwidth = 1.5,
    barheight = 12,
    title.position = "top",
    title.hjust = 0.5
  ))

# Map 2: Average Demand
p2 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors,
    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "viridis",
    name = "Avg\nDemand",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks
    labels = c("0.5", "1.0", "1.5", "2.0", "2.5")
  ) +
  labs(title = "Average Demand",
       subtitle = "Trips per station-hour") +
  mapTheme +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10)
  ) +
  guides(color = guide_colorbar(
    barwidth = 1.5,
    barheight = 12,
    title.position = "top",
    title.hjust = 0.5
  ))




# Map 1: Prediction Errors
p1 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.1) +
  geom_point(
    data = station_errors,
    aes(x = start_lon.y, y = start_lat.x, color = MAE),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "plasma",
    name = "MAE (trips)",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5),
    labels = c("0.5", "1.0", "1.5")
  ) +
  labs(title = "Prediction Errors") +
  mapTheme +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  ) +
  guides(color = guide_colorbar(
    barwidth = 12,
    barheight = 1,
    title.position = "top",
    title.hjust = 0.5
  ))

# Map 2: Average Demand  
p2 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.1) +
  geom_point(
    data = station_errors,
    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "viridis",
    name = "Avg Demand (trips/hour)",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),
    labels = c("0.5", "1.0", "1.5", "2.0", "2.5")
  ) +
  labs(title = "Average Demand") +
  mapTheme +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  ) +
  guides(color = guide_colorbar(
    barwidth = 12,
    barheight = 1,
    title.position = "top",
    title.hjust = 0.5
  ))

# Combine
grid.arrange(
  p1, p2,
  ncol = 2
  )
p1
```

**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?

Yes the prediction errors show spatial clustering.

The model struggles most in the area around Center City, University City, and the surrounding high-activity neighborhoods.

## Temporal Error Patterns

When are we most wrong?

```{r temporal_errors}
# MAE by time of day and day type
temporal_errors <- test %>%
  group_by(time_of_day, weekend) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Prediction Errors by Time Period",
    subtitle = "When is the model struggling most?",
    x = "Time of Day",
    y = "Mean Absolute Error (trips)",
    fill = "Day Type"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Errors and Demographics

Are prediction errors related to neighborhood characteristics?

```{r errors_demographics}
# Join demographic data to station errors
station_errors_demo <- station_errors %>%
  left_join(
    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),
    by = "start_station"
  ) %>%
  filter(!is.na(Med_Inc))

# Create plots
p1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Errors vs. Median Income", x = "Median Income", y = "MAE") +
  plotTheme

p2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Errors vs. Transit Usage", x = "% Taking Transit", y = "MAE") +
  plotTheme

p3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Errors vs. Race", x = "% White", y = "MAE") +
  plotTheme

grid.arrange(p1, p2, p3, ncol = 2)
```

**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?

Prediction errors vary systematically with neighborhood demographics. Stations in higher-income and majority White areas show larger errors, while stations in transit-dependent, lower-income, and more racially diverse neighborhoods have lower errors. 

The model does not disadvantage historically undeserved communities, instead its limitations appear in wealthier areas where rider behavior is more variable.

# Full Error Analysis

Spatial patterns
The errors cluster in and around the busiest, mixed-use parts of the city: Center City, University City, Northern Liberties/Fishtown, and parts of South Philly near Graduate Hospital. These are stations with very high and volatile demand, where the model tends to under-predict during spikes and sometimes over-predict during quieter hours. In contrast, peripheral and more residential areas have lower errors, because demand is smaller and more stable. A reasonable hypothesis is that the model is missing features that explain things like special events, tourism, nightlife, university proximity, detailed land use, and station capacity. Those factors matter most exactly where the errors are largest.

Temporal patterns
Errors are lowest overnight and during very low demand periods, when most stations have 0–2 trips and the model can match them easily. Errors are highest during AM and PM rush hours and some mid-day/evening periods, especially on weekdays, where the model under-predicts high trip counts i.e., 10–20+ trips per station hour. Weekends also show more noise and irregular behavior, making them harder to capture than regular weekday commuting. Within Q2, warmer, nicer days tend to produce larger errors because demand spikes sharply, while cooler or rainy days are easier to predict since overall usage drops and becomes more consistent.

Demographic patterns
When errors are related to census characteristics, they are larger in higher-income, majority-White neighborhoods and smaller in more transit-dependent, lower-income, and racially diverse areas. That means the model does better in communities that are more likely to rely on bikes and transit for everyday mobility, and it struggles more in wealthier areas where ridership is more discretionary, recreational, or event-driven. This means, certain communities are harder to predict but they tend to be higher-income, White neighborhoods, not historically underserved ones. The equity implication is that the model does not appear to worsen access for vulnerable communities based on prediction error alone. Still, if Indego allocates bikes purely based on predicted demand volume, resources could drift toward high-use central/wealthy areas unless equity safeguards like minimum service levels and equity-weighted rules are put in place.

---

# Part 3: Feature Engineering & model improvement

# Create Features
# Rolling 7-Day Average & Perfect Weather

```{r feauture_engineering}

# Rolling 7-day average using lag()
train <- train %>%
  group_by(start_station) %>%
  arrange(interval60) %>%
  mutate(
    lag24  = lag(Trip_Count, 24),
    lag48  = lag(Trip_Count, 48),
    lag72  = lag(Trip_Count, 72),
    lag96  = lag(Trip_Count, 96),
    lag120 = lag(Trip_Count, 120),
    lag144 = lag(Trip_Count, 144),
    lag168 = lag(Trip_Count, 168),
    roll7day = rowMeans(
      cbind(lag24, lag48, lag72, lag96, lag120, lag144, lag168),
      na.rm = TRUE
    )
  ) %>%
  ungroup()

test <- test %>%
  group_by(start_station) %>%
  arrange(interval60) %>%
  mutate(
    lag24  = lag(Trip_Count, 24),
    lag48  = lag(Trip_Count, 48),
    lag72  = lag(Trip_Count, 72),
    lag96  = lag(Trip_Count, 96),
    lag120 = lag(Trip_Count, 120),
    lag144 = lag(Trip_Count, 144),
    lag168 = lag(Trip_Count, 168),
    roll7day = rowMeans(
      cbind(lag24, lag48, lag72, lag96, lag120, lag144, lag168),
      na.rm = TRUE
    )
  ) %>%
  ungroup()

# Perfect biking weather indicator - 60–75°F and no rain
train <- train %>%
  mutate(
    perfect_weather = ifelse(
      Temperature >= 60 & Temperature <= 75 & Precipitation == 0,
      1, 0
    )
  )

test <- test %>%
  mutate(
    perfect_weather = ifelse(
      Temperature >= 60 & Temperature <= 75 & Precipitation == 0,
      1, 0
    )
  )

```


# Fit Model with New Features

```{r feature_improved_model}

model2_improved <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    roll7day + perfect_weather,
  data = train
)

summary(model2_improved)

```

# Predictions and MAE for Improved Model

```{r predictions_mae}

test <- test %>%
  mutate(
    pred2_improved = predict(model2_improved, newdata = test)
  )

mae_improved <- mean(abs(test$Trip_Count - test$pred2_improved), na.rm = TRUE)
mae_improved

```

To improve Model 2, I added two features targeted at the specific error patterns in Q2. These variables were selected because they directly addressed the two biggest error sources observed in Q2, high variability in warm-weather ridership, and medium-term temporal patterns that hourly lags couldn't capture. The improved model reduced MAE from 0.62 to 0.61, with the greatest gains during midday and evening hours where the base model under predicted most. Although the improvement is small, it shows that targeted temporal and weather-based features can make the model more responsive to seasonal changes in Q2.

---

# Part 4: Critical Reflection 

With an MAE of 0.61, the improved Model 2 is accurate enough to support planning and routine operational decision-making, but not precise enough to drive highly time-sensitive rebalancing on its own. Errors are especially significant during peak hours and warm-weather surges, when even small underpredictions can create shortages or empty docks. Because of this, the model should be deployed as a forecasting and scheduling aid, useful for anticipating patterns several hours or days ahead, setting baseline staffing levels, and identifying stations that consistently trend above or below expectations. A realistic deployment would involve using the model for scheduled planning such as setting daily rebalancing priorities or anticipating high-demand periods—while relying on Indego’s existing operational rules to handle short-term fluctuations, as the model can potentialy improve foresight but doesnt over promise precision.

Equity analysis shows that prediction errors are higher in higher-income, majority-White areas and lower in lower-income, transit-dependent, and racially diverse neighborhoods, meaning the model does not disproportionately disadvantage historically undeserved communities. Still, the system could worsen disparities if Indego relies too heavily on predicted demand volume rather than service goals. Wealthier neighborhoods often generate more rides, which could pull resources toward them if decisions are not equity-regulated. To avoid this, Indego should incorporate equity safeguards, such as ensuring minimum service levels at historically underserved stations and weighting rebalancing rules to prevent resource drift. The model also has notable limitations: it lacks information on special events, university features, tourism, and temporary infrastructure changes, and it assumes station-level patterns remain stable across weeks. With more time and data, integrating scheduled event calendars, land-use context, and more flexible modeling approaches would help the system capture these missing patterns and better support both operational reliability and equitable service distribution.

---
